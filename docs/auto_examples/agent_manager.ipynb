{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A demo of Agent Manager\nIn this example, we use the agent manager.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.envs import GridWorld\n\n# A grid world is a simple environment with finite states and actions, on which\n# we can test simple algorithms.\n# -> The reward function can be accessed by: env.R[state, action]\n# -> And the transitions: env.P[state, action, next_state]\nenv_ctor = GridWorld\nenv_kwargs =dict(nrows=3, ncols=10,\n                reward_at = {(1,1):0.1, (2, 9):1.0},\n                walls=((1,4),(2,4), (1,5)),\n                success_probability=0.9)\nenv = env_ctor(**env_kwargs)\n\n\n\nimport numpy as np\nfrom rlberry.agents import AgentWithSimplePolicy\n\nclass ValueIterationAgent(AgentWithSimplePolicy):\n    name = 'ValueIterationAgent'\n    def __init__(self, env, gamma=0.99, epsilon=1e-5, **kwargs):   # it's important to put **kwargs to ensure compatibility with the base class\n        \"\"\"\n        gamma: discount factor\n        episilon: precision of value iteration\n        \"\"\"\n        AgentWithSimplePolicy.__init__(self, env, **kwargs) # self.env is initialized in the base class\n\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.Q = None  # Q function to be computed in fit()\n\n    def fit(self, budget=None, **kwargs):\n        \"\"\"\n        Run value iteration.\n        \"\"\"\n        S, A = env.observation_space.n, env.action_space.n\n        Q = np.zeros((S, A))\n        V = np.zeros(S)\n\n        while True:\n            TQ = np.zeros((S, A))\n            for ss in range(S):\n                for aa in range(A):\n                    TQ[ss, aa] = env.R[ss, aa] + self.gamma*env.P[ss, aa, :].dot(V)\n            V = TQ.max(axis=1)\n\n            if np.abs(TQ-Q).max() < self.epsilon:\n                break\n            Q = TQ\n        self.Q = Q\n\n    def policy(self, observation):\n        return self.Q[observation, :].argmax()\n\n\n    @classmethod\n    def sample_parameters(cls, trial):\n      \"\"\"\n      Sample hyperparameters for hyperparam optimization using Optuna (https://optuna.org/)\n      \"\"\"\n      gamma = trial.suggest_categorical('gamma', [0.1, 0.25, 0.5, 0.75, 0.99])\n      return {'gamma':gamma}\n\n# Create random agent as a baseline\nclass RandomAgent(AgentWithSimplePolicy):\n    name = 'RandomAgent'\n    def __init__(self, env, **kwargs):\n        AgentWithSimplePolicy.__init__(self, env, **kwargs)\n\n    def fit(self, budget=None, **kwargs):\n        pass\n\n    def policy(self, observation):\n        return self.env.action_space.sample()\n\nfrom rlberry.manager import AgentManager, evaluate_agents\n\n# Define parameters\nvi_params = {'gamma':0.1, 'epsilon':1e-3}\n\n# Create AgentManager to fit 4 agents using 1 job\nvi_stats = AgentManager(\n    ValueIterationAgent,\n    (env_ctor, env_kwargs),\n    fit_budget=0,\n    eval_kwargs=dict(eval_horizon=20),\n    init_kwargs=vi_params,\n    n_fit=4)\nvi_stats.fit()\n\n# Create AgentManager for baseline\nbaseline_stats = AgentManager(\n    RandomAgent,\n    (env_ctor, env_kwargs),\n    fit_budget=0,\n    eval_kwargs=dict(eval_horizon=20),\n    n_fit=1)\nbaseline_stats.fit()\n\n# Compare policies using 10 Monte Carlo simulations\noutput = evaluate_agents([vi_stats, baseline_stats], n_simulations=10)\n\nimport matplotlib.pyplot as plt\nplt.plot(np.arange(10), np.arange(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}