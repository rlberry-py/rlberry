{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A demo of using stable baselines with rlberry\nIn this example, we use stable baselines agents in rlberry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.envs import gym_make\nfrom stable_baselines3 import A2C as A2CStableBaselines\nfrom rlberry.agents import AgentWithSimplePolicy\n\n\nclass A2CAgent(AgentWithSimplePolicy):\n    name = 'A2C'\n\n    def __init__(self,\n                 env,\n                 policy,\n                 learning_rate=7e-4,\n                 n_steps: int = 200,\n                 gamma: float = 0.99,\n                 gae_lambda: float = 1.0,\n                 ent_coef: float = 0.0,\n                 vf_coef: float = 0.5,\n                 max_grad_norm: float = 0.5,\n                 rms_prop_eps: float = 1e-5,\n                 use_rms_prop: bool = True,\n                 use_sde: bool = False,\n                 sde_sample_freq: int = -1,\n                 normalize_advantage: bool = False,\n                 tensorboard_log=None,\n                 create_eval_env=False,\n                 policy_kwargs=None,\n                 verbose: int = 0,\n                 seed=None,\n                 device=\"auto\",\n                 _init_setup_model: bool = True,\n                 **kwargs):\n        # init rlberry base class\n        AgentWithSimplePolicy.__init__(self, env, **kwargs)\n        # rlberry accepts tuples (env_constructor, env_kwargs) as env\n        # After a call to __init__, self.env is set as an environment instance\n        env = self.env\n\n        # Generate seed for A2CStableBaselines using rlberry seeding\n        seed = self.rng.integers(2 ** 32).item()\n\n        # init stable baselines class\n        self.wrapped = A2CStableBaselines(\n            policy,\n            env,\n            learning_rate,\n            n_steps,\n            gamma,\n            gae_lambda,\n            ent_coef,\n            vf_coef,\n            max_grad_norm,\n            rms_prop_eps,\n            use_rms_prop,\n            use_sde,\n            sde_sample_freq,\n            normalize_advantage,\n            tensorboard_log,\n            create_eval_env,\n            policy_kwargs,\n            verbose,\n            seed,\n            device,\n            _init_setup_model)\n\n    def fit(self, budget, **kwargs):\n        self.wrapped.learn(total_timesteps=budget, **kwargs)\n\n    def policy(self, observation):\n        action, _ = self.wrapped.predict(observation, deterministic=True)\n        return action\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n        ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n        vf_coef = trial.suggest_uniform(\"vf_coef\", 0, 1)\n        normalize_advantage = trial.suggest_categorical(\"normalize_advantage\", [False, True])\n        return dict(\n            learning_rate=learning_rate,\n            ent_coef=ent_coef,\n            vf_coef=vf_coef,\n            normalize_advantage=normalize_advantage,\n        )\n\n\nif __name__ == '__main__':\n    #\n    # Training one agent\n    #\n    env_ctor = gym_make\n    env_kwargs = dict(id='CartPole-v1')\n    # env = env_ctor(**env_kwargs)\n    # agent = A2CAgent(env, 'MlpPolicy', verbose=1)\n    # agent.fit(budget=1000)\n\n    #\n    # Training several agents and comparing different hyperparams\n    #\n    from rlberry.manager import AgentManager, MultipleManagers, evaluate_agents\n\n    stats = AgentManager(\n        A2CAgent,\n        (env_ctor, env_kwargs),\n        agent_name='A2C baseline',\n        init_kwargs=dict(policy='MlpPolicy', verbose=1),\n        fit_kwargs=dict(log_interval=1000),\n        fit_budget=2500,\n        eval_kwargs=dict(eval_horizon=400),\n        n_fit=4,\n        parallelization='process',\n        output_dir='dev/stable_baselines',\n        seed=123)\n\n    stats_alternative = AgentManager(\n        A2CAgent,\n        (env_ctor, env_kwargs),\n        agent_name='A2C optimized',\n        init_kwargs=dict(policy='MlpPolicy', verbose=1),\n        fit_kwargs=dict(log_interval=1000),\n        fit_budget=2500,\n        eval_kwargs=dict(eval_horizon=400),\n        n_fit=4,\n        parallelization='process',\n        output_dir='dev/stable_baselines',\n        seed=456)\n\n    # Optimize hyperparams (600 seconds)\n    stats_alternative.optimize_hyperparams(\n        timeout=600,\n        n_optuna_workers=2,\n        n_fit=2,\n        optuna_parallelization='process',\n        fit_fraction=1.0)\n\n    # Fit everything in parallel\n    multimanagers = MultipleManagers()\n    multimanagers.append(stats)\n    multimanagers.append(stats_alternative)\n\n    multimanagers.run()\n\n    # Plot policy evaluation\n    out = evaluate_agents(multimanagers.managers)\n    print(out)\n\n    # Visualize policy\n    env = stats_alternative.build_eval_env()\n    agent = stats_alternative.agent_handlers[0]\n    obs = env.reset()\n    for i in range(2500):\n        action = agent.policy(obs)\n        obs, reward, done, info = env.step(action)\n        env.render()\n        if done:\n            break\n    env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}