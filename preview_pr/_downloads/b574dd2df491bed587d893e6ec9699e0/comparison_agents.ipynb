{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compare Bandit Algorithms\n\nThis example illustrate the use of compare_agents, a function that uses multiple-testing to assess whether trained agents are\nstatistically different or not.\n\nRemark that in the case where two agents are not deemed statistically different it can mean either that they are as efficient,\nor it can mean that there have not been enough fits to assess the variability of the agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom rlberry.manager.comparison import compare_agents\nfrom rlberry.manager import AgentManager\nfrom rlberry_research.envs.bandits import BernoulliBandit\nfrom rlberry_research.agents.bandits import (\n    IndexAgent,\n    makeBoundedMOSSIndex,\n    makeBoundedNPTSIndex,\n    makeBoundedUCBIndex,\n    makeETCIndex,\n)\n\n# Parameters of the problem\nmeans = np.array([0.6, 0.6, 0.6, 0.9])  # means of the arms\nA = len(means)\nT = 2000  # Horizon\nN = 50  # number of fits\n\n# Construction of the experiment\n\nenv_ctor = BernoulliBandit\nenv_kwargs = {\"p\": means}\n\n\nclass UCBAgent(IndexAgent):\n    name = \"UCB\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedUCBIndex()\n        IndexAgent.__init__(self, env, index, writer_extra=\"reward\", **kwargs)\n\n\nclass ETCAgent(IndexAgent):\n    name = \"ETC\"\n\n    def __init__(self, env, m=20, **kwargs):\n        index, _ = makeETCIndex(A, m)\n        IndexAgent.__init__(\n            self, env, index, writer_extra=\"action_and_reward\", **kwargs\n        )\n\n\nclass MOSSAgent(IndexAgent):\n    name = \"MOSS\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedMOSSIndex(T, A)\n        IndexAgent.__init__(\n            self, env, index, writer_extra=\"action_and_reward\", **kwargs\n        )\n\n\nclass NPTSAgent(IndexAgent):\n    name = \"NPTS\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedNPTSIndex()\n        IndexAgent.__init__(\n            self,\n            env,\n            index,\n            writer_extra=\"reward\",\n            tracker_params=tracker_params,\n            **kwargs,\n        )\n\n\nAgents_class = [MOSSAgent, NPTSAgent, UCBAgent, ETCAgent]\n\nmanagers = [\n    AgentManager(\n        Agent,\n        train_env=(env_ctor, env_kwargs),\n        fit_budget=T,\n        parallelization=\"process\",\n        mp_context=\"fork\",\n        n_fit=N,\n    )\n    for Agent in Agents_class\n]\n\n\nfor manager in managers:\n    manager.fit()\n\n\ndef eval_function(manager, eval_budget=None, agent_id=0):\n    df = manager.get_writer_data()[agent_id]\n    return T * np.max(means) - np.sum(df.loc[df[\"tag\"] == \"reward\", \"value\"])\n\n\nprint(\n    compare_agents(managers, method=\"tukey_hsd\", eval_function=eval_function, B=10_000)\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}