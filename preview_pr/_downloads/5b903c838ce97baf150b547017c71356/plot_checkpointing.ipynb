{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Checkpointing\n\nThis is a minimal example of how to create checkpoints while training\nyour agents, and how to restore from a previous checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.agents import Agent\nfrom rlberry.manager import ExperimentManager\nfrom rlberry.manager import plot_writer_data\n\n\nclass MyAgent(Agent):\n    name = \"my-agent\"\n\n    def __init__(self, **kwargs):\n        Agent.__init__(self, **kwargs)\n        self.data = [0.0, 1.0]\n        self.checkpoint_file = None\n        self.total_timesteps = 0\n\n    def fit(self, budget: int, **kwargs):\n        \"\"\"This agent is solving a simple difference equation,\n        just to illustrate checkpoints :D\"\"\"\n        del kwargs\n\n        # check if there is a checkpoint to be loaded\n        if self.checkpoint_file is not None:\n            loaded_checkpoint = MyAgent.load(self.checkpoint_file, **self.get_params())\n            self.__dict__.update(loaded_checkpoint.__dict__)\n            print(f\" \\n --> MyAgent loaded from checkpoint: {self.checkpoint_file} \\n\")\n\n        # run training loop\n        for _ in range(budget):\n            self.total_timesteps += 1\n            yt = (2 * self.data[-1] - self.data[-2]) / (1 + 0.01**2)\n            yt += 0.1 * self.rng.normal()\n            self.data.append(yt)\n            if self.writer:\n                self.writer.add_scalar(\"y(t)\", yt, self.total_timesteps)\n\n            # Checkpoint every 500 timesteps\n            if self.total_timesteps % 500 == 0:\n                self.checkpoint_file = self.save(self.output_dir / \"checkpoint.pickle\")\n                print(\n                    f\"checkpoint at {self.checkpoint_file} (timestep = {self.total_timesteps})\"\n                )\n\n    def eval(self, **kwargs):\n        del kwargs\n        return self.data[-1]\n\n\nif __name__ == \"__main__\":\n    manager = ExperimentManager(\n        MyAgent,\n        fit_budget=-1,\n        n_fit=2,\n        seed=123,\n    )\n    # Save manager **before** fit for several timesteps! So that we can see why checkpoints are useful:\n    # even if ExperimentManager is interrupted, it can be loaded and continue training\n    # from last checkpoint.\n    # But first, we need an initial call to ExperimentManager.fit() (for zero or a small number of timesteps),\n    # so that ExperimentManager can instantiate MyAgent and it can start checkpointing itself.\n    # This is because the __init__ method of MyAgent is only executed\n    # after the first call to ExperimentManager.fit().\n    manager.fit(0)\n    manager_file = manager.save()\n    print(f\"\\n Saved manager at {manager_file}.\\n\")\n\n    # Fit for a 1000 timesteps\n    manager.fit(1000)\n\n    # Delete manager. This simulates a situation where we couldn't call\n    # manager.save() after calling manager.fit(), e.g. when your process is interrupted\n    # before .fit() returns.\n    del manager\n\n    # Load manager and continue training from last checkpoint\n    print(f\"\\n Loading manager from {manager_file}.\\n\")\n    loaded_manager = ExperimentManager.load(manager_file)\n    # Fit for 500 more timesteps\n    loaded_manager.fit(500)\n\n    # The plot shows a total of 1500 timesteps!\n    plot_writer_data(loaded_manager, tag=\"y(t)\", show=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}