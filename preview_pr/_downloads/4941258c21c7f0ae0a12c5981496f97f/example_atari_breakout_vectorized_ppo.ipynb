{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A demo of ATARI Breakout environment with vectorized PPOAgent\nIllustration of the training and video rendering of PPO Agent in\nATARI Breakout environment.\n\nAgent is slightly tuned, but not optimal. This is just for illustration purpose.\n\n.. video:: ../../example_plot_atari_breakout_vectorized_ppo.mp4\n   :width: 600\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.manager import ExperimentManager\nfrom datetime import datetime\nfrom rlberry_research.agents.torch import PPOAgent\nfrom gymnasium.wrappers.record_video import RecordVideo\nimport shutil\nimport os\nfrom rlberry.envs.gym_make import atari_make\nfrom rlberry_research.agents.torch.utils.training import model_factory_from_env\n\n\ninitial_time = datetime.now()\nprint(\"-------- init agent --------\")\n\n\npolicy_mlp_configs = {\n    \"type\": \"MultiLayerPerceptron\",  # A network architecture\n    \"layer_sizes\": [512],  # Network dimensions\n    \"reshape\": False,\n    \"is_policy\": True,  # The network should output a distribution\n    # over actions\n}\n\ncritic_mlp_configs = {\n    \"type\": \"MultiLayerPerceptron\",\n    \"layer_sizes\": [512],\n    \"reshape\": False,\n    \"out_size\": 1,  # The critic network is an approximator of\n    # a value function V: States -> |R\n}\n\npolicy_configs = {\n    \"type\": \"ConvolutionalNetwork\",  # A network architecture\n    \"activation\": \"RELU\",\n    \"in_channels\": 4,\n    \"in_height\": 84,\n    \"in_width\": 84,\n    \"head_mlp_kwargs\": policy_mlp_configs,\n    \"transpose_obs\": False,\n    \"is_policy\": True,  # The network should output a distribution\n}\n\ncritic_configs = {\n    \"type\": \"ConvolutionalNetwork\",\n    \"layer_sizes\": \"RELU\",\n    \"in_channels\": 4,\n    \"in_height\": 84,\n    \"in_width\": 84,\n    \"head_mlp_kwargs\": critic_mlp_configs,\n    \"transpose_obs\": False,\n    \"out_size\": 1,\n}\n\n\ntuned_xp = ExperimentManager(\n    PPOAgent,  # The Agent class.\n    (\n        atari_make,\n        dict(id=\"ALE/Breakout-v5\"),\n    ),  # The Environment to solve.\n    init_kwargs=dict(  # Where to put the agent's hyperparameters\n        batch_size=256,\n        optimizer_type=\"ADAM\",  # What optimizer to use for policy gradient descent steps.\n        learning_rate=2.5e-4,  # Size of the policy gradient descent steps.\n        policy_net_fn=model_factory_from_env,  # A policy network constructor\n        policy_net_kwargs=policy_configs,  # Policy network's architecure\n        value_net_fn=model_factory_from_env,  # A Critic network constructor\n        value_net_kwargs=critic_configs,  # Critic network's architecure.\n        n_envs=8,\n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_eps=0.1,\n        k_epochs=4,\n        n_steps=128,\n    ),\n    fit_budget=10_000_000,  # The number of interactions between the agent and the environment during training.\n    eval_kwargs=dict(\n        eval_horizon=500\n    ),  # The number of interactions between the agent and the environment during evaluations.\n    n_fit=1,  # The number of agents to train. Usually, it is good to do more than 1 because the training is stochastic.\n    agent_name=\"PPO_tuned\",  # The agent's name.\n    output_dir=\"PPO_for_breakout\",\n)\n\nprint(\"-------- init agent : done!--------\")\nprint(\"-------- train agent --------\")\n\ntuned_xp.fit()\n\nprint(\"-------- train agent : done!--------\")\n\nfinal_train_time = datetime.now()\n\nprint(\"-------- test agent with video--------\")\n\nenv = atari_make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\nenv = RecordVideo(env, \"_video/temp\")\n\nif \"render_modes\" in env.metadata:\n    env.metadata[\"render.modes\"] = env.metadata[\n        \"render_modes\"\n    ]  # bug with some 'gym' version\n\nobservation, info = env.reset()\nfor tt in range(30000):\n    action = tuned_xp.get_agent_instances()[0].policy(observation)\n    observation, reward, terminated, truncated, info = env.step(action)\n    done = terminated or truncated\n    if done:\n        break\n\nenv.close()\n\nprint(\"-------- test agent with video : done!--------\")\nfinal_test_time = datetime.now()\ntuned_xp.save()\n\n# need to move the final result inside the folder used for documentation\nos.rename(\n    \"_video/temp/rl-video-episode-0.mp4\",\n    \"_video/example_plot_atari_breakout_vectorized_ppo.mp4\",\n)\nshutil.rmtree(\"_video/temp/\")\n\n\nprint(\"Done!!!\")\nprint(\"-------------\")\nprint(\"begin run at :\" + str(initial_time))\nprint(\"end training at :\" + str(final_train_time))\nprint(\"end run at :\" + str(final_test_time))\nprint(\"-------------\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}