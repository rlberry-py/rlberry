{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A demo of Bandit BAI on a real dataset to select mirrors\nIn this exemple we use a sequential halving agent to find the best server\nto download ubuntu from among a choice of 8 french servers.\n\nThe quirck of this application is that there is a possible timeout when pinging\na server. We handle this by using the median instead of the mean in sequential\nhalving's objective.\n\nThe code is in three parts: definition of environment, definition of agent,\nand finally definition of the experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom rlberry.manager import ExperimentManager, read_writer_data\nfrom rlberry.envs.interface import Model\nfrom rlberry_research.agents.bandits import BanditWithSimplePolicy\nimport rlberry.spaces as spaces\n\nimport requests\nimport matplotlib.pyplot as plt\n\n\nimport rlberry\n\nlogger = rlberry.logger\n\n# Environment definition\n\n\nTIMEOUT = 2\n\nmirrors_ubuntu = np.array(\n    [\n        \"https://ubuntu.lafibre.info/ubuntu/\",\n        \"https://mirror.ubuntu.ikoula.com/\",\n        \"http://ubuntu.mirrors.ovh.net/ubuntu/\",\n        \"http://miroir.univ-lorraine.fr/ubuntu/\",\n        \"http://ubuntu.univ-nantes.fr/ubuntu/\",\n        \"https://ftp.u-picardie.fr/mirror/ubuntu/ubuntu/\",\n        \"http://ubuntu.univ-reims.fr/ubuntu/\",\n        \"http://www-ftp.lip6.fr/pub/linux/distributions/Ubuntu/archive/\",\n    ]\n)\n\n\ndef get_time(url):\n    try:\n        resp = requests.get(url, timeout=TIMEOUT)\n        return resp.elapsed.total_seconds()\n    except:\n        return np.inf  # timeout\n\n\nclass MirrorBandit(Model):\n    \"\"\"\n    Real environment for bandit problems.\n    The reward is the response time for French servers meant to download ubuntu.\n\n    On action i, gives a negative waiting time to reach url i in mirror_ubuntu.\n\n    WARNING : if there is a timeout when querying the mirror, will result in\n    a negative infinite reward.\n\n    Parameters\n    ----------\n    url_ids : list of int or None,\n        list of ids used to select a subset of the url list provided in the source.\n        if None, all the urls are selected (i.e. 8 arms bandits).\n    \"\"\"\n\n    name = \"MirrorEnv\"\n\n    def __init__(self, url_ids=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        if url_ids:\n            self.url_list = mirrors_ubuntu[url_ids]\n        else:\n            self.url_list = mirrors_ubuntu\n\n        self.n_arms = len(self.url_list)\n        self.action_space = spaces.Discrete(self.n_arms)\n\n    def step(self, action):\n        \"\"\"\n        Sample the reward associated to the action.\n        \"\"\"\n        # test that the action exists\n        assert action < self.n_arms\n\n        reward = -get_time(self.url_list[action])\n        terminated = True\n        truncated = False\n        return 0, reward, terminated, truncated, {}\n\n    def reset(self, seed=None):\n        \"\"\"\n        Reset the environment to a default state.\n        \"\"\"\n        return 0, {}\n\n\nenv_ctor = MirrorBandit\nenv_kwargs = {}\n\n# BAI Agent definition\n\n\nclass SeqHalvAgent(BanditWithSimplePolicy):\n    \"\"\"\n    Sequential Halving Agent\n    \"\"\"\n\n    name = \"SeqHalvAgent\"\n\n    def __init__(self, env, **kwargs):\n        BanditWithSimplePolicy.__init__(\n            self, env, writer_extra=\"action_and_reward\", **kwargs\n        )\n\n    def fit(self, budget=None, **kwargs):\n        horizon = budget\n        rewards = []\n        actions = []\n        active_set = np.arange(self.n_arms)\n\n        logk = int(np.ceil(np.log2(self.n_arms)))\n        ep = 0\n\n        for r in range(logk):\n            tr = np.floor(horizon / (len(active_set) * logk))\n            for _ in range(int(tr)):\n                for k in active_set:\n                    action = k\n                    actions += [action]\n                    observation, reward, terminated, truncated, info = self.env.step(\n                        action\n                    )\n                    rewards += [reward]\n                    ep += 1\n            reward_est = [\n                np.median(np.array(rewards)[actions == k]) for k in active_set\n            ]\n            # We estimate the reward using the median instead of the mean to\n            # handle timeout.\n            half_len = int(np.ceil(len(active_set) / 2))\n            active_set = active_set[np.argsort(reward_est)[-half_len:]]\n\n        self.optimal_action = active_set[0]\n        self.writer.add_scalar(\"optimal_action\", self.optimal_action, ep)\n\n        return actions\n\n\n# Experiment\n\nxp_manager = ExperimentManager(\n    SeqHalvAgent,\n    (env_ctor, env_kwargs),\n    fit_budget=100,  # we use only 100 iterations for faster example run in doc.\n    n_fit=1,\n    agent_name=\"SH\",\n)\nxp_manager.fit()\n\nrewards = read_writer_data([xp_manager], preprocess_tag=\"reward\")[\"value\"]\nactions = read_writer_data([xp_manager], preprocess_tag=\"action\")[\"value\"]\n\n\nplt.boxplot([-rewards[actions == a] for a in range(6)])\nplt.xlabel(\"Server\")\nplt.ylabel(\"Waiting time (in s)\")\nplt.show()\n\nprint(\n    \"The optimal action (fastest server) is server number \",\n    xp_manager.agent_handlers[0].optimal_action + 1,\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}