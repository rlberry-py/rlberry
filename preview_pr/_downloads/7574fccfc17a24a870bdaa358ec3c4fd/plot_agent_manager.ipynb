{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A demo of Experiment Manager\nIn this example, we use the ExperimentManager.\n\nFirst, we initialize a grid world environment with finite state space and actions.\nA grid world is a simple environment with finite states and actions, on which\nwe can test simple algorithms. The reward function can be accessed by: ``env.R[state, action]``, while the transitions by: ``env.P[state, action, next_state]``.\n\nThen, we implement a value iteration algorithm for the action values:\n\n\\begin{align}Q(s, a) \\leftarrow \\sum_{s^{\\prime}} p(s'|a, s)\\left( R(s, a)+\\gamma \\max _{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}) \\right).\\end{align}\n\nFinally, we compare with a baseline provided by a random policy using the ExperimentManager class which trains, evaluates and gathers statistics about the two agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry_research.envs import GridWorld\n\n# Create a grid world environment and an agent with a value iteration policy\nenv_ctor = GridWorld\nenv_kwargs = dict(\n    nrows=3,\n    ncols=10,\n    reward_at={(1, 1): 0.1, (2, 9): 1.0},\n    walls=((1, 4), (2, 4), (1, 5)),\n    success_probability=0.9,\n)\nenv = env_ctor(**env_kwargs)\n\nimport numpy as np\nfrom rlberry.agents import AgentWithSimplePolicy\n\n\nclass ValueIterationAgent(AgentWithSimplePolicy):\n    name = \"ValueIterationAgent\"\n\n    def __init__(\n        self, env, gamma=0.99, epsilon=1e-5, **kwargs\n    ):  # it's important to put **kwargs to ensure compatibility with the base class\n        \"\"\"\n        gamma: discount factor\n        episilon: precision of value iteration\n        \"\"\"\n        AgentWithSimplePolicy.__init__(\n            self, env, **kwargs\n        )  # self.env is initialized in the base class\n\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.Q = None  # Q function to be computed in fit()\n\n    def fit(self, budget=None, **kwargs):\n        \"\"\"\n        Run value iteration.\n        \"\"\"\n        S, A = env.observation_space.n, env.action_space.n\n        Q = np.zeros((S, A))\n        V = np.zeros(S)\n\n        while True:\n            TQ = np.zeros((S, A))\n            for ss in range(S):\n                for aa in range(A):\n                    TQ[ss, aa] = env.R[ss, aa] + self.gamma * env.P[ss, aa, :].dot(V)\n            V = TQ.max(axis=1)\n\n            if np.abs(TQ - Q).max() < self.epsilon:\n                break\n            Q = TQ\n        self.Q = Q\n\n    def policy(self, observation):\n        return self.Q[observation, :].argmax()\n\n    @classmethod\n    def sample_parameters(cls, trial):\n        \"\"\"\n        Sample hyperparameters for hyperparam optimization using Optuna (https://optuna.org/)\n        \"\"\"\n        gamma = trial.suggest_categorical(\"gamma\", [0.1, 0.25, 0.5, 0.75, 0.99])\n        return {\"gamma\": gamma}\n\n\n# Create random agent as a baseline\nclass RandomAgent(AgentWithSimplePolicy):\n    name = \"RandomAgent\"\n\n    def __init__(self, env, **kwargs):\n        AgentWithSimplePolicy.__init__(self, env, **kwargs)\n\n    def fit(self, budget=None, **kwargs):\n        pass\n\n    def policy(self, observation):\n        return self.env.action_space.sample()\n\n\nfrom rlberry.manager import ExperimentManager, evaluate_agents\n\n# Define parameters\nvi_params = {\"gamma\": 0.1, \"epsilon\": 1e-3}\n\n# Create ExperimentManager to fit 4 agents using 1 job\nvi_stats = ExperimentManager(\n    ValueIterationAgent,\n    (env_ctor, env_kwargs),\n    fit_budget=0,\n    eval_kwargs=dict(eval_horizon=20),\n    init_kwargs=vi_params,\n    n_fit=4,\n)\nvi_stats.fit()\n\n# Create ExperimentManager for baseline\nbaseline_stats = ExperimentManager(\n    RandomAgent,\n    (env_ctor, env_kwargs),\n    fit_budget=0,\n    eval_kwargs=dict(eval_horizon=20),\n    n_fit=1,\n)\nbaseline_stats.fit()\n\n# Compare policies using 10 Monte Carlo simulations\noutput = evaluate_agents([vi_stats, baseline_stats], n_simulations=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}