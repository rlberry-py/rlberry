{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison subplots of various index based bandits algorithms\n\nThis script Compare several bandits agents and as a sub-product also shows\nhow to use subplots in with `plot_writer_data`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom rlberry_research.envs.bandits import BernoulliBandit\nfrom rlberry.manager import ExperimentManager, plot_writer_data\nfrom rlberry_research.agents.bandits import (\n    IndexAgent,\n    RandomizedAgent,\n    makeBoundedIMEDIndex,\n    makeBoundedMOSSIndex,\n    makeBoundedNPTSIndex,\n    makeBoundedUCBIndex,\n    makeBoundedUCBVIndex,\n    makeETCIndex,\n    makeEXP3Index,\n)\n\n# Agents definition\n\n\n# Parameters of the problem\nmeans = np.array([0.6, 0.6, 0.6, 0.9])  # means of the arms\nA = len(means)\nT = 2000  # Horizon\nM = 10  # number of MC simu\n\n# Construction of the experiment\n\nenv_ctor = BernoulliBandit\nenv_kwargs = {\"p\": means}\n\n\nclass UCBAgent(IndexAgent):\n    name = \"UCB\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedUCBIndex()\n        IndexAgent.__init__(\n            self, env, index, writer_extra=\"action_and_reward\", **kwargs\n        )\n\n\nclass UCBVAgent(IndexAgent):\n    name = \"UCBV\"\n\n    def __init__(self, env, **kwargs):\n        index, params = makeBoundedUCBVIndex()\n        IndexAgent.__init__(\n            self,\n            env,\n            index,\n            writer_extra=\"action_and_reward\",\n            tracker_params=params,\n            **kwargs\n        )\n\n\nclass ETCAgent(IndexAgent):\n    name = \"ETC\"\n\n    def __init__(self, env, m=20, **kwargs):\n        index, _ = makeETCIndex(A, m)\n        IndexAgent.__init__(\n            self, env, index, writer_extra=\"action_and_reward\", **kwargs\n        )\n\n\nclass MOSSAgent(IndexAgent):\n    name = \"MOSS\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedMOSSIndex(T, A)\n        IndexAgent.__init__(\n            self, env, index, writer_extra=\"action_and_reward\", **kwargs\n        )\n\n\nclass IMEDAgent(IndexAgent):\n    name = \"IMED\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedIMEDIndex()\n        IndexAgent.__init__(\n            self,\n            env,\n            index,\n            writer_extra=\"action_and_reward\",\n            tracker_params=tracker_params,\n            **kwargs\n        )\n\n\nclass NPTSAgent(IndexAgent):\n    name = \"NPTS\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedNPTSIndex()\n        IndexAgent.__init__(\n            self,\n            env,\n            index,\n            writer_extra=\"action_and_reward\",\n            tracker_params=tracker_params,\n            **kwargs\n        )\n\n\nclass EXP3Agent(RandomizedAgent):\n    name = \"EXP3\"\n\n    def __init__(self, env, **kwargs):\n        prob, tracker_params = makeEXP3Index()\n        RandomizedAgent.__init__(\n            self,\n            env,\n            prob,\n            writer_extra=\"action_and_reward\",\n            tracker_params=tracker_params,\n            **kwargs\n        )\n\n\nAgents_class = [\n    ETCAgent,\n    EXP3Agent,\n    IMEDAgent,\n    MOSSAgent,\n    NPTSAgent,\n    UCBAgent,\n    UCBVAgent,\n]\n\nagents = [\n    ExperimentManager(\n        Agent,\n        (env_ctor, env_kwargs),\n        fit_budget=T,\n        n_fit=M,\n        parallelization=\"process\",\n        mp_context=\"fork\",\n        seed=42,\n    )\n    for Agent in Agents_class\n]\n\n# these parameters should give parallel computing even in notebooks\n\n\n# Agent training\nfor agent in agents:\n    agent.fit()\n\n\n# Compute and plot regret\ndef compute_regret(rewards):\n    return np.cumsum(np.max(means) - rewards)\n\n\n# Compute and plot (pseudo-)regret\ndef compute_pseudo_regret(actions):\n    return np.cumsum(np.max(means) - means[actions.astype(int)])\n\n\noutput = plot_writer_data(\n    agents,\n    tag=\"action\",\n    preprocess_func=compute_pseudo_regret,\n    title=\"Cumulative Pseudo-Regret\",\n    linestyles=True,\n)\n\noutput = plot_writer_data(\n    agents,\n    tag=\"reward\",\n    preprocess_func=compute_regret,\n    title=\"Cumulative Regret\",\n    linestyles=True,\n)\n\n\n# Compute and plot number of times each arm was selected\ndef compute_na(actions, a):\n    return np.cumsum(actions == a)\n\n\nfig, axes = plt.subplots(2, 2, sharey=True, figsize=(6, 6))\naxes = axes.ravel()\nfor arm in range(A):\n    output = plot_writer_data(\n        agents,\n        tag=\"action\",\n        preprocess_func=lambda actions: compute_na(actions, arm),\n        title=\"Na for arm \" + str(arm) + \", mean=\" + str(means[arm]),\n        ax=axes[arm],\n        show=False,\n        linestyles=True,\n    )\nfig.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}