{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using multiple virtual environments with rlberry\n\nThis example illustrate how to use the \"with_venv\" decorator\nin order to automatically construct and use virtual environments\nfor RL experimentation with several separated environments.\n\nThe decorator `with_venv` is used to generate scripts at compile time\nand then are run via `run_venv_xp`.\nRemark: the functions 'run_sb' and 'run_mushroom' are not directly called\nand are only there to give the script's text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.manager import with_venv, run_venv_xp\n\n\n# Decorator with_venv will create a script to be run in the virtual environment with\n# the libraries in the import_libs list. Here we want to create a virtual environment\n# containing mushroom_rl library and run an example script taken from mushroom_rl doc.\n@with_venv(import_libs=[\"numpy\", \"mushroom_rl\"], venv_dir_name=\"rlberry_venvs\")\ndef run_mushroom():\n    \"\"\"\n    Simple script to solve a simple chain with Q-Learning.\n\n    \"\"\"\n    import numpy as np\n\n    from mushroom_rl.algorithms.value import QLearning\n    from mushroom_rl.core import Core, Logger\n    from mushroom_rl.environments import generate_simple_chain\n    from mushroom_rl.policy import EpsGreedy\n    from mushroom_rl.utils.parameters import Parameter\n    from mushroom_rl.utils.dataset import compute_J\n\n    np.random.seed()\n\n    logger = Logger(QLearning.__name__, results_dir=None)\n    logger.strong_line()\n    logger.info(\"Experiment Algorithm: \" + QLearning.__name__)\n\n    # MDP\n    mdp = generate_simple_chain(state_n=5, goal_states=[2], prob=0.8, rew=1, gamma=0.9)\n\n    # Policy\n    epsilon = Parameter(value=0.15)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    learning_rate = Parameter(value=0.2)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = QLearning(mdp.info, pi, **algorithm_params)\n\n    # Core\n    core = Core(agent, mdp)\n\n    # Initial policy Evaluation\n    dataset = core.evaluate(n_steps=1000)\n    J = np.mean(compute_J(dataset, mdp.info.gamma))\n    logger.info(f\"J start: {J}\")\n\n    # Train\n    core.learn(n_steps=10000, n_steps_per_fit=1)\n\n    # Final Policy Evaluation\n    dataset = core.evaluate(n_steps=1000)\n    J = np.mean(compute_J(dataset, mdp.info.gamma))\n    logger.info(f\"J final: {J}\")\n\n\n# Here we want to create a virtual environment containing stable-baselines3 library\n# and run an example script taken from stable-baselines3 doc.\n@with_venv(\n    import_libs=[\"stable-baselines3\"], venv_dir_name=\"rlberry_venvs\", python_ver=\"3.9\"\n)\ndef run_sb():\n    import gymnasium as gym\n\n    from stable_baselines3 import A2C\n\n    env = gym.make(\"CartPole-v1\")\n\n    model = A2C(\"MlpPolicy\", env, verbose=1)\n    model.learn(total_timesteps=1_500)\n\n    vec_env = model.get_env()\n    obs = vec_env.reset()\n    cum_reward = 0\n    for i in range(1000):\n        action, _state = model.predict(obs, deterministic=True)\n        obs, reward, done, info = vec_env.step(action)\n        cum_reward += reward\n    print(cum_reward)\n\n\nif __name__ == \"__main__\":\n    # Collect all the scripts from the directory rlberry_venvs and tun them.\n    run_venv_xp(venv_dir_name=\"rlberry_venvs\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}