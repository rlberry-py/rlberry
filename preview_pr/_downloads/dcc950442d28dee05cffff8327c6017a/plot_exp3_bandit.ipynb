{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# EXP3 Bandit cumulative regret\n\nThis script shows how to define a bandit environment and an EXP3\nrandomized algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom rlberry_research.envs.bandits import AdversarialBandit\nfrom rlberry_research.agents.bandits import (\n    RandomizedAgent,\n    TSAgent,\n    makeEXP3Index,\n    makeBetaPrior,\n)\nfrom rlberry.manager import ExperimentManager, plot_writer_data\n\n\n# Agents definition\n\n\nclass EXP3Agent(RandomizedAgent):\n    name = \"EXP3\"\n\n    def __init__(self, env, **kwargs):\n        prob, tracker_params = makeEXP3Index()\n        RandomizedAgent.__init__(\n            self,\n            env,\n            prob,\n            writer_extra=\"action\",\n            tracker_params=tracker_params,\n            **kwargs\n        )\n\n\nclass BernoulliTSAgent(TSAgent):\n    \"\"\"Thompson sampling for Bernoulli bandit\"\"\"\n\n    name = \"TS\"\n\n    def __init__(self, env, **kwargs):\n        prior, _ = makeBetaPrior()\n        TSAgent.__init__(self, env, prior, writer_extra=\"action\", **kwargs)\n\n\n# Parameters of the problem\nT = 3000  # Horizon\nM = 20  # number of MC simu\n\n\ndef switching_rewards(T, gap=0.1, rate=1.6):\n    \"\"\"Adversarially switching rewards over exponentially long phases.\n    Inspired by Zimmert, Julian, and Yevgeny Seldin.\n    \"Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits.\"\n    J. Mach. Learn. Res. 22 (2021): 28-1.\n    \"\"\"\n    rewards = np.zeros((T, 2))\n    t = 0\n    exp = 1\n    high_rewards = True\n    for t in range(T):\n        if t > np.floor(rate**exp):\n            high_rewards = not high_rewards\n            exp += 1\n        if high_rewards:\n            rewards[t] = [1.0 - gap, 1.0]\n        else:\n            rewards[t] = [0.0, gap]\n    return rewards\n\n\nrewards = switching_rewards(T, rate=5.0)\n\n\n# Construction of the experiment\n\nenv_ctor = AdversarialBandit\nenv_kwargs = {\"rewards\": rewards}\n\nAgents_class = [EXP3Agent, BernoulliTSAgent]\n\nagents = [\n    ExperimentManager(\n        Agent,\n        (env_ctor, env_kwargs),\n        init_kwargs={},\n        fit_budget=T,\n        n_fit=M,\n        parallelization=\"process\",\n        mp_context=\"fork\",\n    )\n    for Agent in Agents_class\n]\n\n# these parameters should give parallel computing even in notebooks\n\n\n# Agent training\nfor agent in agents:\n    agent.fit()\n\n\n# Compute and plot (pseudo-)regret\ndef compute_pseudo_regret(actions):\n    selected_rewards = np.array(\n        [rewards[t, int(action)] for t, action in enumerate(actions)]\n    )\n    return np.cumsum(np.max(rewards, axis=1) - selected_rewards)\n\n\noutput = plot_writer_data(\n    agents,\n    tag=\"action\",\n    preprocess_func=compute_pseudo_regret,\n    title=\"Cumulative Pseudo-Regret\",\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}