

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>Comparison of Agents &mdash; rlberry 0.7.3.post16.dev0+8710009 documentation</title>
  
  <link rel="canonical" href="https://rlberry-py.github.io/rlberry/basics/comparison.html" />

  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/js/vendor/jquery-3.6.3.slim.min.js"></script> 
</head>
<body>


<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../index.html">Home</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../installation.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Examples</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../api.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../changelog.html">Changelog</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../about.html">About</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="https://github.com/rlberry-py/rlberry">GitHub</a>
        </li>
        <!--
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
          </div>
        </li>-->
    </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
            <form action="https://duckduckgo.com/">
            <input type="hidden" id="sites" name="sites" value="https://rlberry-py.github.io/rlberry/">
            <input type="search" placeholder="Search &hellip;" value="" name="q" />
            <input class="sk-search-text-btn" type="submit" value="Go" /></form>

          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
        </div>
	<br>
        <div class="alert alert-warning p-1 mb-2" role="alert">

          <p class="text-center mb-0">
          rlberry 0.7.3.post16.dev0+8710009<br/>
          <a href="../versions.html">Other versions</a>
          </p>

        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">Comparison of Agents</a><ul>
<li><a class="reference internal" href="#quick-reminder-on-hypothesis-testing">Quick reminder on hypothesis testing</a><ul>
<li><a class="reference internal" href="#two-sample-testing">Two sample testing</a></li>
<li><a class="reference internal" href="#multiple-testing">Multiple testing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multiple-agent-comparison-in-rlberry">Multiple agent comparison in rlberry</a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="comparison-of-agents">
<span id="comparison-page"></span><h1>Comparison of Agents<a class="headerlink" href="#comparison-of-agents" title="Permalink to this heading">¶</a></h1>
<p>The performance of one execution of a Deep RL algorithm is random so that independent executions are needed to assess it precisely.
In this section we use multiple hypothesis testing to assert that we used enough fits to be able to say that the agents are indeed differents and that the perceived differences are not just a result of randomness.</p>
<section id="quick-reminder-on-hypothesis-testing">
<h2>Quick reminder on hypothesis testing<a class="headerlink" href="#quick-reminder-on-hypothesis-testing" title="Permalink to this heading">¶</a></h2>
<section id="two-sample-testing">
<h3>Two sample testing<a class="headerlink" href="#two-sample-testing" title="Permalink to this heading">¶</a></h3>
<p>In its most simple form, a statistical test is aimed at deciding, whether a given collection of data <span class="math notranslate nohighlight">\(X_1,\dots,X_N\)</span> adheres to some hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> (called the null hypothesis), or if it is a better fit for an alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span>.</p>
<p>Consider two samples <span class="math notranslate nohighlight">\(X_1,\dots,X_N\)</span> and <span class="math notranslate nohighlight">\(Y_1,\dots,Y_N\)</span> and do a two-sample test deciding whether the mean of the distribution of the <span class="math notranslate nohighlight">\(X_i\)</span>’s is equal to the mean of the distribution of the <span class="math notranslate nohighlight">\(Y_i\)</span>’s:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
H_0 : \mathbb{E}[X] = \mathbb{E}[Y] \quad \text{vs}\quad H_1: \mathbb{E}[X] \neq \mathbb{E}[Y]
\end{equation*}\]</div>
<p>In both cases, the result of a test is either accept <span class="math notranslate nohighlight">\(H_0\)</span> or reject <span class="math notranslate nohighlight">\(H_0\)</span>. This answer is not a ground truth: there is some probability that we make an error. However, this  probability of error is often controlled and can be decomposed in type I error and type II errors (often denoted <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> respectively).</p>
<center>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(H_0\)</span> is true</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(H_0\)</span> is false</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>We accept <span class="math notranslate nohighlight">\(H_0\)</span></p></td>
<td><p>No error</p></td>
<td><p>type II error <span class="math notranslate nohighlight">\(\beta\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>We reject <span class="math notranslate nohighlight">\(H_0\)</span></p></td>
<td><p>type I error <span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
<td><p>No error</p></td>
</tr>
</tbody>
</table>
</center>
<p>Note that the problem is not symmetric: failing to reject the null hypothesis does not mean that the null hypothesis is true.  It can be that there is not enough data to reject <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
</section>
<section id="multiple-testing">
<h3>Multiple testing<a class="headerlink" href="#multiple-testing" title="Permalink to this heading">¶</a></h3>
<p>When doing simultaneously several statistical tests, one must be careful that the error of each test accumulate and if one is not cautious, the overall error may become non-negligible. As a consequence, multiple strategies have been developed to deal with multiple testing problem.</p>
<p>To deal with the multiple testing problem, the first step is to define what is an error. There are several definitions of error in multiple testing, one possibility is the Family-wise error which is defined as the probability to make at least one false rejection (at least one type I error):</p>
<p><div class="math notranslate nohighlight">
\[\mathrm{FWE} = \mathbb{P}_{H_j, j \in \textbf{I}}\left(\exists j \in \textbf{I}:\quad  \text{reject }H_j \right),\]</div>
</p>
<p>where <span class="math notranslate nohighlight">\(\mathbb{P}_{H_j, j \in \textbf{I}}\)</span> is used to denote the probability when <span class="math notranslate nohighlight">\(\textbf{I}\)</span> is the set of indices of the hypotheses that are actually true (and <span class="math notranslate nohighlight">\(\textbf{I}^c\)</span> the set of hypotheses that are actually false).</p>
</section>
</section>
<section id="multiple-agent-comparison-in-rlberry">
<h2>Multiple agent comparison in rlberry<a class="headerlink" href="#multiple-agent-comparison-in-rlberry" title="Permalink to this heading">¶</a></h2>
<p>We compute the performances of one agent as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">rlberry.envs</span> <span class="kn">import</span> <span class="n">gym_make</span>
<span class="kn">from</span> <span class="nn">rlberry.agents.stable_baselines</span> <span class="kn">import</span> <span class="n">StableBaselinesAgent</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">A2C</span>
<span class="kn">from</span> <span class="nn">rlberry.manager</span> <span class="kn">import</span> <span class="n">AgentManager</span><span class="p">,</span> <span class="n">evaluate_agents</span>

<span class="n">env_ctor</span> <span class="o">=</span> <span class="n">gym_make</span>
<span class="n">env_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>

<span class="n">n_simulations</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_fit</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">rbagent</span> <span class="o">=</span> <span class="n">AgentManager</span><span class="p">(</span>
    <span class="n">StableBaselinesAgent</span><span class="p">,</span>
    <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
    <span class="n">init_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algo_cls</span><span class="o">=</span><span class="n">A2C</span><span class="p">),</span>  <span class="c1"># Init value for StableBaselinesAgent</span>
    <span class="n">agent_name</span><span class="o">=</span><span class="s2">&quot;A2CAgent&quot;</span><span class="p">,</span>
    <span class="n">fit_budget</span><span class="o">=</span><span class="mf">3e4</span><span class="p">,</span>
    <span class="n">eval_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_horizon</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span>
    <span class="n">n_fit</span><span class="o">=</span><span class="n">n_fit</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">rbagent</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>  <span class="c1"># get 5 trained agents</span>
<span class="n">performances</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rbagent</span><span class="o">.</span><span class="n">eval_agents</span><span class="p">(</span><span class="n">n_simulations</span><span class="p">,</span> <span class="n">agent_id</span><span class="o">=</span><span class="n">idx</span><span class="p">))</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
<p>We begin by training all the agents (here <span class="math notranslate nohighlight">\(8\)</span> agents). Then, we evaluate each trained agent <code class="docutils literal notranslate"><span class="pre">n_simulations</span></code> times (here <span class="math notranslate nohighlight">\(50\)</span>). The performance of one trained agent is then the mean of its evaluations. We do this for each agent that we trained, obtaining <code class="docutils literal notranslate"><span class="pre">n_fit</span></code> evaluation performances. These <code class="docutils literal notranslate"><span class="pre">n_fit</span></code> numbers are the random numbers that will be used to do hypothesis testing.</p>
<p>The evaluation and statistical hypothesis testing is handled through the function <a class="reference internal" href="../generated/rlberry.manager.compare_agents.html#rlberry.manager.compare_agents" title="rlberry.manager.compare_agents"><code class="xref py py-class docutils literal notranslate"><span class="pre">compare_agents</span></code></a>.</p>
<p>For example we may compare PPO, A2C and DQNAgent on Cartpole with the following code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rlberry.agents.stable_baselines</span> <span class="kn">import</span> <span class="n">StableBaselinesAgent</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">A2C</span><span class="p">,</span> <span class="n">PPO</span><span class="p">,</span> <span class="n">DQN</span>
<span class="kn">from</span> <span class="nn">rlberry.manager.comparison</span> <span class="kn">import</span> <span class="n">compare_agents</span>

<span class="n">agents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AgentManager</span><span class="p">(</span>
        <span class="n">StableBaselinesAgent</span><span class="p">,</span>
        <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
        <span class="n">init_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algo_cls</span><span class="o">=</span><span class="n">A2C</span><span class="p">),</span>  <span class="c1"># Init value for StableBaselinesAgent</span>
        <span class="n">agent_name</span><span class="o">=</span><span class="s2">&quot;A2CAgent&quot;</span><span class="p">,</span>
        <span class="n">fit_budget</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span>
        <span class="n">eval_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_horizon</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span>
        <span class="n">n_fit</span><span class="o">=</span><span class="n">n_fit</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">AgentManager</span><span class="p">(</span>
        <span class="n">StableBaselinesAgent</span><span class="p">,</span>
        <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
        <span class="n">init_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algo_cls</span><span class="o">=</span><span class="n">PPO</span><span class="p">),</span>  <span class="c1"># Init value for StableBaselinesAgent</span>
        <span class="n">agent_name</span><span class="o">=</span><span class="s2">&quot;PPOAgent&quot;</span><span class="p">,</span>
        <span class="n">fit_budget</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span>
        <span class="n">eval_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_horizon</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span>
        <span class="n">n_fit</span><span class="o">=</span><span class="n">n_fit</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">AgentManager</span><span class="p">(</span>
        <span class="n">StableBaselinesAgent</span><span class="p">,</span>
        <span class="p">(</span><span class="n">env_ctor</span><span class="p">,</span> <span class="n">env_kwargs</span><span class="p">),</span>
        <span class="n">init_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algo_cls</span><span class="o">=</span><span class="n">DQN</span><span class="p">),</span>  <span class="c1"># Init value for StableBaselinesAgent</span>
        <span class="n">agent_name</span><span class="o">=</span><span class="s2">&quot;DQNAgent&quot;</span><span class="p">,</span>
        <span class="n">fit_budget</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span>
        <span class="n">eval_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_horizon</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span>
        <span class="n">n_fit</span><span class="o">=</span><span class="n">n_fit</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">agents</span><span class="p">:</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">compare_agents</span><span class="p">(</span><span class="n">agents</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span>       <span class="n">Agent1</span> <span class="n">vs</span> <span class="n">Agent2</span>  <span class="n">mean</span> <span class="n">Agent1</span>  <span class="n">mean</span> <span class="n">Agent2</span>  <span class="n">mean</span> <span class="n">diff</span>    <span class="n">std</span> <span class="n">diff</span> <span class="n">decisions</span>     <span class="n">p</span><span class="o">-</span><span class="n">val</span> <span class="n">significance</span>
<span class="mi">0</span>  <span class="n">A2CAgent</span> <span class="n">vs</span> <span class="n">PPOAgent</span>     <span class="mf">416.9975</span>    <span class="mf">500.00000</span>  <span class="o">-</span><span class="mf">83.00250</span>  <span class="mf">147.338488</span>    <span class="n">accept</span>  <span class="mf">0.266444</span>
<span class="mi">1</span>  <span class="n">A2CAgent</span> <span class="n">vs</span> <span class="n">DQNAgent</span>     <span class="mf">416.9975</span>    <span class="mf">260.38375</span>  <span class="mf">156.61375</span>  <span class="mf">179.503659</span>    <span class="n">reject</span>  <span class="mf">0.017001</span>            <span class="o">*</span>
<span class="mi">2</span>  <span class="n">PPOAgent</span> <span class="n">vs</span> <span class="n">DQNAgent</span>     <span class="mf">500.0000</span>    <span class="mf">260.38375</span>  <span class="mf">239.61625</span>   <span class="mf">80.271521</span>    <span class="n">reject</span>  <span class="mf">0.000410</span>          <span class="o">***</span>
</pre></div>
</div>
<p>The results of <code class="docutils literal notranslate"><span class="pre">compare_agents(agents)</span></code> show the p-values and significance level if the method is tukey_hsd and  it shows the decision accept or reject of the test with Family-wise error controlled by <span class="math notranslate nohighlight">\(0.05\)</span>. In our case, we see that DQN is worse than A2C and PPO but the difference between PPO and A2C is not statistically significant. Remark that no significance (which is to say, decision to accept <span class="math notranslate nohighlight">\(H_0\)</span>) does not necessarily mean that the algorithms perform the same, it can be that there is not enough data (and it is likely that it is the case here).</p>
<p><em>Remark</em>: the comparison we do here is a black-box comparison in the sense that we don’t care how the algorithms were tuned or how many training steps are used, we suppose that the user already tuned these parameters adequately for a fair comparison.</p>
<p><em>Remark 2</em>: the comparison we do here is non-adaptive. To go further and use as little seeds as possible, look at <a class="reference internal" href="userguide/adastop.html#adastop-userguide"><span class="std std-ref">AdaStop</span></a>.</p>
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2023, rlberry team.
          <a href="../_sources/basics/comparison.md.txt" rel="nofollow">Show this page source</a>
      </footer>
    </div>
  </div>
</div>
<script src="../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>