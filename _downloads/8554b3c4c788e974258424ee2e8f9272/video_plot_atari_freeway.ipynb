{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A demo of ATARI Freeway environment with DQNAgent\nIllustration of the training and video rendering of DQN Agent in\nATARI Freeway environment.\n\nAgent is slightly tuned, but not optimal. This is just for illustration purpose.\n\n.. video:: ../../video_plot_atari_freeway.mp4\n   :width: 600\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.manager import ExperimentManager\nfrom datetime import datetime\nfrom rlberry_research.agents.torch.dqn.dqn import DQNAgent\nfrom gymnasium.wrappers.rendering import RecordVideo\nimport shutil\nimport os\nfrom rlberry.envs.gym_make import atari_make\n\n\ninitial_time = datetime.now()\nprint(\"-------- init agent --------\")\n\nmlp_configs = {\n    \"type\": \"MultiLayerPerceptron\",  # A network architecture\n    \"layer_sizes\": [512],  # Network dimensions\n    \"reshape\": False,\n    \"is_policy\": False,  # The network should output a distribution\n    # over actions\n}\n\ncnn_configs = {\n    \"type\": \"ConvolutionalNetwork\",  # A network architecture\n    \"activation\": \"RELU\",\n    \"in_channels\": 4,\n    \"in_height\": 84,\n    \"in_width\": 84,\n    \"head_mlp_kwargs\": mlp_configs,\n    \"transpose_obs\": False,\n    \"is_policy\": False,  # The network should output a distribution\n}\n\ntuned_xp = ExperimentManager(\n    DQNAgent,  # The Agent class.\n    (\n        atari_make,\n        dict(\n            id=\"ALE/Freeway-v5\",\n        ),\n    ),  # The Environment to solve.\n    init_kwargs=dict(  # Where to put the agent's hyperparameters\n        q_net_constructor=\"rlberry_research.agents.torch.utils.training.model_factory_from_env\",\n        q_net_kwargs=cnn_configs,\n        max_replay_size=50000,\n        batch_size=32,\n        learning_starts=25000,\n        gradient_steps=1,\n        epsilon_final=0.01,\n        learning_rate=1e-4,  # Size of the policy gradient descent steps.\n        chunk_size=1,\n    ),\n    fit_budget=90000,  # The number of interactions between the agent and the environment during training.\n    eval_kwargs=dict(\n        eval_horizon=500\n    ),  # The number of interactions between the agent and the environment during evaluations.\n    n_fit=1,  # The number of agents to train. Usually, it is good to do more than 1 because the training is stochastic.\n    agent_name=\"DQN_tuned\",  # The agent's name.\n    output_dir=\"DQN_for_freeway\",\n)\n\nprint(\"-------- init agent : done!--------\")\nprint(\"-------- train agent --------\")\n\ntuned_xp.fit()\n\nprint(\"-------- train agent : done!--------\")\n\nfinal_train_time = datetime.now()\n\nprint(\"-------- test agent with video--------\")\n\nenv = atari_make(\"ALE/Freeway-v5\", render_mode=\"rgb_array\")\nenv = RecordVideo(env, \"_video/temp\")\n\nif \"render_modes\" in env.metadata:\n    env.metadata[\"render.modes\"] = env.metadata[\n        \"render_modes\"\n    ]  # bug with some 'gym' version\n\nobservation, info = env.reset()\nfor tt in range(30000):\n    action = tuned_xp.get_agent_instances()[0].policy(observation)\n    observation, reward, terminated, truncated, info = env.step(action)\n    done = terminated or truncated\n    if done:\n        break\n\nenv.close()\n\nprint(\"-------- test agent with video : done!--------\")\nfinal_test_time = datetime.now()\ntuned_xp.save()\n\n# need to move the final result inside the folder used for documentation\nos.rename(\"_video/temp/rl-video-episode-0.mp4\", \"_video/video_plot_atari_freeway.mp4\")\nshutil.rmtree(\"_video/temp/\")\n\n\nprint(\"Done!!!\")\nprint(\"-------------\")\nprint(\"begin run at :\" + str(initial_time))\nprint(\"end training at :\" + str(final_train_time))\nprint(\"end run at :\" + str(final_test_time))\nprint(\"-------------\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}