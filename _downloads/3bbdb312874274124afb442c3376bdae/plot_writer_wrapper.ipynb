{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Record reward during training and then plot it\n\nThis script shows how to modify an agent to easily record reward or action\nduring the fit of the agent and then use the plot utils.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you already ran this script once, the fitted agent has been saved\n    in rlberry_data folder. Then, you can comment-out the line\n\n```python\nagent.fit(budget=10)\n```\n    and avoid fitting the agent one more time, the statistics from the last\n    time you fitted the agent will automatically be loaded. See\n    `rlberry.manager.plot_writer_data` documentation for more information.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom rlberry_scool.envs import GridWorld\nfrom rlberry.manager import plot_writer_data, ExperimentManager\nfrom rlberry_scool.agents import UCBVIAgent\nimport matplotlib.pyplot as plt\n\n# We wrape the default writer of the agent in a WriterWrapper\n# to record rewards.\n\n\nclass VIAgent(UCBVIAgent):\n    name = \"UCBVIAgent\"\n\n    def __init__(self, env, **kwargs):\n        UCBVIAgent.__init__(self, env, writer_extra=\"reward\", horizon=50, **kwargs)\n\n\nenv_ctor = GridWorld\nenv_kwargs = dict(\n    nrows=3,\n    ncols=10,\n    reward_at={(1, 1): 0.1, (2, 9): 1.0},\n    walls=((1, 4), (2, 4), (1, 5)),\n    success_probability=0.7,\n)\n\nenv = env_ctor(**env_kwargs)\nxp_manager = ExperimentManager(VIAgent, (env_ctor, env_kwargs), fit_budget=10, n_fit=3)\n\nxp_manager.fit(budget=10)\n# comment the line above if you only want to load data from rlberry_data.\n\n\n# We use the following preprocessing function to plot the cumulative reward.\ndef compute_reward(rewards):\n    return np.cumsum(rewards)\n\n\n# Plot of the cumulative reward.\noutput = plot_writer_data(\n    xp_manager, tag=\"reward\", preprocess_func=compute_reward, title=\"Cumulative Reward\"\n)\n# The output is for 500 global steps because it uses 10 fit_budget * horizon\n\n# Log-Log plot :\nfig, ax = plt.subplots(1, 1)\nplot_writer_data(\n    xp_manager,\n    tag=\"reward\",\n    preprocess_func=compute_reward,\n    title=\"Cumulative Reward\",\n    ax=ax,\n    show=False,  # necessary to customize axes\n)\nax.set_xlim(100, 500)\nax.relim()\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}